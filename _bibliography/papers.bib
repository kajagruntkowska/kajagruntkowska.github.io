---
---

@string{aps = {American Physical Society,}}

@article{riabinin2025gluon,
  title={Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)},
  author={Riabinin, Artem and Shulgin, Egor and Gruntkowska, Kaja and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2505.13416},
  year={2025},
  url={https://arxiv.org/abs/2505.13416},
  html={https://arxiv.org/abs/2505.13416},
  selected={true},
  abstract={Recent developments in deep learning optimization have brought about radically new algorithms based on the Linear Minimization Oracle (LMO) framework, such as Muon and Scion. After over a decade of ùñ†ùñΩùñ∫ùóÜ's dominance, these LMO-based methods are emerging as viable replacements, offering several practical advantages such as improved memory efficiency, better hyperparameter transferability, and most importantly, superior empirical performance on large-scale tasks, including LLM training. However, a significant gap remains between their practical use and our current theoretical understanding: prior analyses (1) overlook the layer-wise LMO application of these optimizers in practice, and (2) rely on an unrealistic smoothness assumption, leading to impractically small stepsizes. To address both, we propose a new LMO-based method called Gluon, capturing prior theoretically analyzed methods as special cases, and introduce a new refined generalized smoothness model that captures the layer-wise geometry of neural networks, matches the layer-wise practical implementation of Muon and Scion, and leads to convergence guarantees with strong practical predictive power. Unlike prior results, our theoretical stepsizes closely match the fine-tuned values. Our experiments with NanoGPT and CNN confirm that our assumption holds along the optimization trajectory, ultimately closing the gap between theory and practice.}
}

@article{gruntkowska2025ball,
  title={The Ball-Proximal (="Broximal") Point Method: a New Algorithm, Convergence Theory, and Applications},
  author={Gruntkowska, Kaja and Li, Hanmin and Rane, Aadi and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2502.02002},
  year={2025},
  url={https://arxiv.org/abs/2502.02002},
  html={https://arxiv.org/abs/2502.02002},
  selected={true},
  slides={BPM_slides.pdf},
  abstract={Non-smooth and non-convex global optimization poses significant challenges across various applications, where standard gradient-based methods often struggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or Ball Point Method (BPM) for short -- a novel algorithmic framework inspired by the classical Proximal Point Method (PPM), which, as we show, sheds new light on several foundational optimization paradigms and phenomena, including non-convex and non-smooth optimization, acceleration, smoothing, adaptive stepsize selection, and trust-region methods. At the core of BPM lies the ball-proximal ("broximal") operator, which arises from the classical proximal operator by replacing the quadratic distance penalty by a ball constraint. Surprisingly, and in sharp contrast with the sublinear rate of PPM in the nonsmooth convex regime, we prove that BPM converges linearly and in a finite number of steps in the same regime. Furthermore, by introducing the concept of ball-convexity, we prove that BPM retains the same global convergence guarantees under weaker assumptions, making it a powerful tool for a broader class of potentially non-convex optimization problems. Just like PPM plays the role of a conceptual method inspiring the development of practically efficient algorithms and algorithmic elements, e.g., gradient descent, adaptive step sizes, acceleration, and "W" in AdamW, we believe that BPM should be understood in the same manner: as a blueprint and inspiration for further development.}
}

@article{anyszka2024tighter,
  title={Tighter performance theory of FedExProx},
  author={Anyszka, Wojciech and Gruntkowska, Kaja and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2410.15368},
  year={2024},
  url={https://arxiv.org/abs/2410.15368},
  html={https://arxiv.org/abs/2410.15368},
  selected={true},
  abstract={We revisit FedExProx -- a recently proposed distributed optimization method designed to enhance convergence properties of parallel proximal algorithms via extrapolation. In the process, we uncover a surprising flaw: its known theoretical guarantees on quadratic optimization tasks are no better than those offered by the vanilla Gradient Descent (GD) method. Motivated by this observation, we develop a novel analysis framework, establishing a tighter linear convergence rate for non-strongly convex quadratic problems. By incorporating both computation and communication costs, we demonstrate that FedExProx can indeed provably outperform GD, in stark contrast to the original analysis. Furthermore, we consider partial participation scenarios and analyze two adaptive extrapolation strategies -- based on gradient diversity and Polyak stepsizes -- again significantly outperforming previous results. Moving beyond quadratics, we extend the applicability of our analysis to general functions satisfying the Polyak-≈Åojasiewicz condition, outperforming the previous strongly convex analysis while operating under weaker assumptions. Backed by empirical results, our findings point to a new and stronger potential of FedExProx, paving the way for further exploration of the benefits of extrapolation in federated learning.}
}

@article{tyurin2024freya,
  title={Freya page: First optimal time complexity for large-scale nonconvex finite-sum optimization with heterogeneous asynchronous computations},
  author={Tyurin, Alexander and Gruntkowska, Kaja and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2024},
  url={https://arxiv.org/abs/2405.15545},
  html={https://arxiv.org/abs/2405.15545},
  selected={true},
  slides={Freya_PAGE_slides.pdf},
  abstract={In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times. We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations. By being robust to "stragglers" and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods. Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit. This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime.}
}

@article{gruntkowska2024improving,
  title={Improving the worst-case bidirectional communication complexity for nonconvex distributed optimization under function similarity},
  author={Gruntkowska, Kaja and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2024},
  url={https://arxiv.org/abs/2402.06412},
  html={https://arxiv.org/abs/2402.06412},
  selected={true},
  slides={MARINA_P_slides.pdf},
  abstract={Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing the server-to-worker communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analyses demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression. We introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number of workers increases. Theoretical findings align closely with empirical experiments, underscoring the efficiency of the proposed algorithms.}
}

@inproceedings{rammal2024communication,
  title={Communication compression for byzantine robust learning: New efficient algorithms and improved rates},
  author={Rammal, Ahmad and Gruntkowska, Kaja and Fedin, Nikita and Gorbunov, Eduard and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1207--1215},
  year={2024},
  organization={PMLR},
  url={https://arxiv.org/abs/2310.09804},
  html={https://arxiv.org/abs/2310.09804},
  abstract={Byzantine robustness is an essential feature of algorithms for certain distributed optimization problems, typically encountered in collaborative/federated learning. These problems are usually huge-scale, implying that communication compression is also imperative for their resolution. These factors have spurred recent algorithmic and theoretical developments in the literature of Byzantine-robust learning with compression. In this paper, we contribute to this research area in two main directions. First, we propose a new Byzantine-robust method with compression -- Byz-DASHA-PAGE -- and prove that the new method has better convergence rate (for non-convex and Polyak-≈Åojasiewicz smooth optimization problems), smaller neighborhood size in the heterogeneous case, and tolerates more Byzantine workers under over-parametrization than the previous method with SOTA theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the first Byzantine-robust method with communication compression and error feedback -- Byz-EF21 -- along with its bidirectional compression version -- Byz-EF21-BC -- and derive the convergence rates for these methods for non-convex and Polyak-≈Åojasiewicz smooth case. We test the proposed methods and illustrate our theoretical findings in the numerical experiments.}
}

@inproceedings{gruntkowska2023ef21,
  title={EF21-P and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression},
  author={Gruntkowska, Kaja and Tyurin, Alexander and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11761--11807},
  year={2023},
  organization={PMLR},
  url={https://arxiv.org/abs/2209.15218},
  html={https://arxiv.org/abs/2209.15218},
  abstract={In this work we focus our attention on distributed optimization problems in the context where the communication time between the server and the workers is non-negligible. We obtain novel methods supporting bidirectional compression (both from the server to the workers and vice versa) that enjoy new state-of-the-art theoretical communication complexity for convex and nonconvex problems. Our bounds are the first that manage to decouple the variance/error coming from the workers-to-server and server-to-workers compression, transforming a multiplicative dependence to an additive one. Moreover, in the convex regime, we obtain the first bounds that match the theoretical communication complexity of gradient descent. Even in this convex regime, our algorithms work with biased gradient estimators, which is non-standard and requires new proof techniques that may be of independent interest. Finally, our theoretical results are corroborated through suitable experiments.}
}