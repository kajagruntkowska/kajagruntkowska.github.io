<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    Kaja Gruntkowska
  
</title>
<meta name="author" content="Kaja Gruntkowska">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://kajagruntkowska.github.io/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>










  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/publications/">Publications
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/cv/">CV
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/talks/">Talks
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">
      
        <span class="font-weight-bold">Kaja</span> 
        Gruntkowska
      
    </h1>
    <p class="desc"></p>
  </header>

  <article>
    
      <div class="profile float-right">
        
          
          
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="/assets/img/prof_pic.jpg?ad199fd17712df120e781b753a572770" class="img-fluid z-depth-1
      rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
        
          <div class="more-info">
<p>Building 12, 4700 KAUST</p> <p>Thuwal, Saudi Arabia</p>
</div>
        
      </div>
    

    <div class="clearfix">
<p>I am a second-year PhD student at the KAUST Center of Excellence for Generative AI, supervised by Prof. <a href="https://richtarik.org/" rel="external nofollow noopener" target="_blank">Peter Richt√°rik</a>. My research focuses on developing the algorithmic and mathematical foundations of randomized optimization, with a particular emphasis on distributed computing. I work on designing practically motivated algorithms with provable convergence guarantees, bridging theory and real-world applications to advance scalable machine learning.</p>

<p>I hold a Bachelor‚Äôs degree in Mathematics and Statistics from the <a href="https://warwick.ac.uk/study/undergraduate/courses/bsc-mathematics-statistics/" rel="external nofollow noopener" target="_blank">University of Warwick</a> (2022) and a Master‚Äôs in Statistical Science from the <a href="https://www.ox.ac.uk/admissions/graduate/courses/msc-statistical-science" rel="external nofollow noopener" target="_blank">University of Oxford</a> (2023).</p>
</div>

    <!-- News -->
    

    <!-- Latest posts -->
    

    <!-- Selected papers -->
    
      <h2>
        <a href="/publications/" style="color: inherit">Recent publications</a>
      </h2>
      <div class="publications">
  <ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="riabinin2025gluon" class="col-sm-8">
    <!-- Title -->
    <div class="title">Gluon: Making Muon &amp; Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)</div>
    <!-- Author -->
    <div class="author">
      

      
      Artem
            Riabinin, Egor
            Shulgin, <em>Kaja
            Gruntkowska</em>, and Peter
            Richt√°rik
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv preprint arXiv:2505.13416</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/abs/2505.13416" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Recent developments in deep learning optimization have brought about radically new algorithms based on the Linear Minimization Oracle (LMO) framework, such as Muon and Scion. After over a decade of ùñ†ùñΩùñ∫ùóÜ‚Äôs dominance, these LMO-based methods are emerging as viable replacements, offering several practical advantages such as improved memory efficiency, better hyperparameter transferability, and most importantly, superior empirical performance on large-scale tasks, including LLM training. However, a significant gap remains between their practical use and our current theoretical understanding: prior analyses (1) overlook the layer-wise LMO application of these optimizers in practice, and (2) rely on an unrealistic smoothness assumption, leading to impractically small stepsizes. To address both, we propose a new LMO-based method called Gluon, capturing prior theoretically analyzed methods as special cases, and introduce a new refined generalized smoothness model that captures the layer-wise geometry of neural networks, matches the layer-wise practical implementation of Muon and Scion, and leads to convergence guarantees with strong practical predictive power. Unlike prior results, our theoretical stepsizes closely match the fine-tuned values. Our experiments with NanoGPT and CNN confirm that our assumption holds along the optimization trajectory, ultimately closing the gap between theory and practice.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="gruntkowska2025ball" class="col-sm-8">
    <!-- Title -->
    <div class="title">The Ball-Proximal (="Broximal") Point Method: a New Algorithm, Convergence Theory, and Applications</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Kaja
            Gruntkowska</em>, Hanmin
            Li, Aadi
            Rane, and Peter
            Richt√°rik
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv preprint arXiv:2502.02002</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/abs/2502.02002" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
        
          <a href="/assets/pdf/BPM_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Non-smooth and non-convex global optimization poses significant challenges across various applications, where standard gradient-based methods often struggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or Ball Point Method (BPM) for short ‚Äì a novel algorithmic framework inspired by the classical Proximal Point Method (PPM), which, as we show, sheds new light on several foundational optimization paradigms and phenomena, including non-convex and non-smooth optimization, acceleration, smoothing, adaptive stepsize selection, and trust-region methods. At the core of BPM lies the ball-proximal ("broximal") operator, which arises from the classical proximal operator by replacing the quadratic distance penalty by a ball constraint. Surprisingly, and in sharp contrast with the sublinear rate of PPM in the nonsmooth convex regime, we prove that BPM converges linearly and in a finite number of steps in the same regime. Furthermore, by introducing the concept of ball-convexity, we prove that BPM retains the same global convergence guarantees under weaker assumptions, making it a powerful tool for a broader class of potentially non-convex optimization problems. Just like PPM plays the role of a conceptual method inspiring the development of practically efficient algorithms and algorithmic elements, e.g., gradient descent, adaptive step sizes, acceleration, and "W" in AdamW, we believe that BPM should be understood in the same manner: as a blueprint and inspiration for further development.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="anyszka2024tighter" class="col-sm-8">
    <!-- Title -->
    <div class="title">Tighter performance theory of FedExProx</div>
    <!-- Author -->
    <div class="author">
      

      
      Wojciech
            Anyszka, <em>Kaja
            Gruntkowska</em>, Alexander
            Tyurin, and Peter
            Richt√°rik
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv preprint arXiv:2410.15368</em>,  2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/abs/2410.15368" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We revisit FedExProx ‚Äì a recently proposed distributed optimization method designed to enhance convergence properties of parallel proximal algorithms via extrapolation. In the process, we uncover a surprising flaw: its known theoretical guarantees on quadratic optimization tasks are no better than those offered by the vanilla Gradient Descent (GD) method. Motivated by this observation, we develop a novel analysis framework, establishing a tighter linear convergence rate for non-strongly convex quadratic problems. By incorporating both computation and communication costs, we demonstrate that FedExProx can indeed provably outperform GD, in stark contrast to the original analysis. Furthermore, we consider partial participation scenarios and analyze two adaptive extrapolation strategies ‚Äì based on gradient diversity and Polyak stepsizes ‚Äì again significantly outperforming previous results. Moving beyond quadratics, we extend the applicability of our analysis to general functions satisfying the Polyak-≈Åojasiewicz condition, outperforming the previous strongly convex analysis while operating under weaker assumptions. Backed by empirical results, our findings point to a new and stronger potential of FedExProx, paving the way for further exploration of the benefits of extrapolation in federated learning.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="tyurin2024freya" class="col-sm-8">
    <!-- Title -->
    <div class="title">Freya page: First optimal time complexity for large-scale nonconvex finite-sum optimization with heterogeneous asynchronous computations</div>
    <!-- Author -->
    <div class="author">
      

      
      Alexander
            Tyurin, <em>Kaja
            Gruntkowska</em>, and Peter
            Richt√°rik
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Advances in Neural Information Processing Systems</em>,  2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/abs/2405.15545" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
        
          <a href="/assets/pdf/Freya_PAGE_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times. We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations. By being robust to "stragglers" and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods. Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit. This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="gruntkowska2024improving" class="col-sm-8">
    <!-- Title -->
    <div class="title">Improving the worst-case bidirectional communication complexity for nonconvex distributed optimization under function similarity</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Kaja
            Gruntkowska</em>, Alexander
            Tyurin, and Peter
            Richt√°rik
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Advances in Neural Information Processing Systems</em>,  2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/abs/2402.06412" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
        
          <a href="/assets/pdf/MARINA_P_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing the server-to-worker communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analyses demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression. We introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number of workers increases. Theoretical findings align closely with empirical experiments, underscoring the efficiency of the proposed algorithms.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
</div>

    

    <!-- Social -->
    
      <div class="social">
        <div class="contact-icons">
  
      <a href="mailto:%6B%61%6A%61.%67%72%75%6E%74%6B%6F%77%73%6B%61@%6B%61%75%73%74.%65%64%75.%73%61" title="email"><i class="fa-solid fa-envelope"></i></a>
    

  
      <a href="https://www.linkedin.com/in/kaja-gruntkowska" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a>
    

  
      <a href="https://scholar.google.com/citations?user=H0W8ADAAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
    

  
      <a href="https://orcid.org/0009-0002-0144-7018" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
    

</div>
      </div>
    

    
  </article>
</div>

      
    </div>

    <!-- Footer -->
    


  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      
  ¬© Copyright 2025
  Kaja
  
  Gruntkowska.
  
  
    Last updated: May 20, 2025.
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>























  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->




  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  









  <!-- Scrolling Progress Bar -->
  <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>






  </body>
</html>
